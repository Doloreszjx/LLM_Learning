#!/usr/bin/python
# -*- coding:utf-8 -*-
# Bert Position ï¼ˆå¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼‰
# InputEmbedding = TokenEmbedding + SegmentEmbedding + PositionEmbedding
# TokenEmbedding: æ¯ä¸ªè¯æˆ–è€…wordpieceçš„å‘é‡è¡¨ç¤º
# SegmentEmbeddingï¼šåŒºåˆ†æ¯ä¸ªå¥å­
# PositionEmbeddingï¼šè¡¨ç¤ºæ¯ä¸ªtokençš„ä½ç½®ï¼ˆ0ï¼Œ1ï¼Œ2ï¼Œâ€¦â€¦ï¼‰
import math

import torch
import torch.nn as nn
import torch.nn.functional as F

class BertEmbedding(nn.Module):
    def __init__(self, vocab_size, embedding_dim, max_length=512, segment_vocab_size=2):
        super(BertEmbedding, self).__init__()
        self.vocab_size = vocab_size
        self.token_embed = nn.Embedding(vocab_size, embedding_dim)
        self.position_embed = nn.Embedding(max_length, embedding_dim)
        self.segment_embed = nn.Embedding(segment_vocab_size, embedding_dim)
        self.layer_norm = nn.LayerNorm(embedding_dim)

    def forward(self, token_ids, segment_ids):
        # token_ids: [batch_size, seq_len]
        seq_len = token_ids.size(-1)
        # ç”Ÿæˆå’Œåºåˆ—é•¿åº¦ä¸€è‡´çš„ä½ç½®å¼ é‡
        position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0).expand_as(token_ids)

        position_embeddings = self.position_embed(position_ids)
        token_embeddings = self.token_embed(token_ids)
        segment_embeddings = self.segment_embed(segment_ids)

        embeddings = token_embeddings + position_embeddings + segment_embeddings

        return self.layer_norm(embeddings)

class MultiBertAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiBertAttention, self).__init__()
        assert d_model % num_heads == 0
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads

        # ä¸‰ä¸ªçº¿æ€§å˜æ¢ï¼šQ, K, V
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x):
        # x: [batch_size, seq_len, d_model]
        batch_size, seq_len, d_model = x.size()
        H = self.num_heads

        # Q,K,V: [batch_size, seq_len, d_model]
        Q = self.W_q(x)
        K = self.W_k(x)
        V = self.W_v(x)

        # æ‹†åˆ†æˆå¤šå¤´ï¼š[batch_size, H, seq_len, d_model/H]
        Q = Q.view(batch_size, seq_len, H, -1).transpose(1, 2)
        K = K.view(batch_size, seq_len, H, -1).transpose(1, 2)
        V = V.view(batch_size, seq_len, H, -1).transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attention_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attention_weights, V)

        # è½¬å›åŸå½¢çŠ¶: [batch_size, seq_len, d_model]
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)

        return self.W_o(output)

# æ¨¡æ‹Ÿè¾“å…¥ï¼šbatch_size=2ï¼Œåºåˆ—é•¿åº¦=5
batch_size, seq_len = 2, 5
vocab_size = 1000
embed_size = 32
num_heads = 4

# æ„é€ å‡è¾“å…¥
token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
segment_ids = torch.zeros_like(token_ids)  # å…¨éƒ¨å±äºå¥å­A

# åˆå§‹åŒ–æ¨¡å—
embedding_layer = BertEmbedding(vocab_size, embed_size, max_length=seq_len)
attention_layer = MultiBertAttention(embed_size, num_heads)

# å‰å‘ä¼ æ’­
embeddings = embedding_layer(token_ids, segment_ids)
attention_output = attention_layer(embeddings)

print("ğŸ”¹ Token IDs:\n", token_ids)
print("\nğŸ”¹ Embedding è¾“å‡ºå½¢çŠ¶:", embeddings.shape)
print("ğŸ”¹ Attention è¾“å‡ºå½¢çŠ¶:", attention_output.shape)



