from tokenizers import ByteLevelBPETokenizer
import os

os.makedirs("byte_bpe_demo", exist_ok=True)

# åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ–‡æœ¬æ–‡ä»¶ä½œä¸ºè¯­æ–™
with open("tiny_corpus.txt", "w", encoding="utf-8") as f:
    f.write("Hello, world!\nI love pizza ğŸ•\nGPT is amazing ğŸš€")

# åˆå§‹åŒ– tokenizer
tokenizer = ByteLevelBPETokenizer()

# è®­ç»ƒä¸€ä¸ªå°æ¨¡å‹ï¼ˆçœŸå®è®­ç»ƒä¸€èˆ¬ç”¨å‡ ç™¾ä¸‡æ¡æ•°æ®ï¼‰
tokenizer.train(
    files="tiny_corpus.txt",
    vocab_size=200,
    min_frequency=1,
    special_tokens=["<s>", "<pad>", "</s>", "<unk>", "<mask>"]
)

# ä¿å­˜æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
tokenizer.save_model("byte_bpe_demo")

# åŠ è½½åˆšåˆšè®­ç»ƒçš„ tokenizer
tokenizer = ByteLevelBPETokenizer(
    "byte_bpe_demo/vocab.json",
    "byte_bpe_demo/merges.txt"
)

# è¦æµ‹è¯•çš„å¥å­
text = "Hello, pizza! ğŸ•ğŸš€"

# ç¼–ç æˆ token
encoded = tokenizer.encode(text)
print("ğŸ”¢ Token IDs:", encoded.ids)
print("ğŸ§© Tokens:", encoded.tokens)

# è§£ç å›åŸæ–‡
decoded = tokenizer.decode(encoded.ids)
print("ğŸ” Decoded text:", decoded)
